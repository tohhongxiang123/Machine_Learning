{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # first we load all our data from mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET DIGIT OCCURENCES\n",
      "0:5923\n",
      "1:6742\n",
      "2:5958\n",
      "3:6131\n",
      "4:5842\n",
      "5:5421\n",
      "6:5918\n",
      "7:6265\n",
      "8:5851\n",
      "9:5949\n",
      "\n",
      "TEST SET DIGIT OCCURENCES\n",
      "0:980\n",
      "1:1135\n",
      "2:1032\n",
      "3:1010\n",
      "4:982\n",
      "5:892\n",
      "6:958\n",
      "7:1028\n",
      "8:974\n",
      "9:1009\n"
     ]
    }
   ],
   "source": [
    "print('TRAINING SET DIGIT OCCURENCES')\n",
    "for i in range(10):\n",
    "    print('{}:{}'.format(i,np.bincount(y_train)[i])) # gives the occurence of values\n",
    "print('\\nTEST SET DIGIT OCCURENCES')\n",
    "for i in range(10):\n",
    "    print('{}:{}'.format(i,np.bincount(y_test)[i])) # gives the occurence of values\n",
    "# we make sure we do not have a heavily skewed distribution of digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING VARIABLES\n",
    "IMG_WIDTH = 28\n",
    "IMG_HEIGHT = 28\n",
    "IMG_CHANNELS = 1\n",
    "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)\n",
    "\n",
    "NUMBER_OF_TRAIN = len(X_train)\n",
    "NUMBER_OF_TEST = len(X_test)\n",
    "NUMBER_OF_CLASSES = 10\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 733,722\n",
      "Trainable params: 733,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 213,994\n",
      "Trainable params: 213,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Activation, Dropout, Input\n",
    "\n",
    "######### WE GONNA BUILD MULTIPLE MODELS AND SEE HOW THEY FARE AGAINST ONE ANOTHER ##########\n",
    "\n",
    "# dense layers model #\n",
    "modeldense = Sequential()\n",
    "modeldense.add(Flatten(input_shape=(28,28,1))) \n",
    "# need to flatten input because each sample is 3d, and our dense layer accepts 1d samples \n",
    "\n",
    "modeldense.add(Dense(784))\n",
    "modeldense.add(Activation('relu'))\n",
    "modeldense.add(Dropout(0.4))\n",
    "\n",
    "modeldense.add(Dense(128))\n",
    "modeldense.add(Activation('relu'))\n",
    "\n",
    "modeldense.add(Dense(128))\n",
    "modeldense.add(Activation('relu'))\n",
    "modeldense.add(Dropout(0.4))\n",
    "\n",
    "modeldense.add(Dense(10))\n",
    "modeldense.add(Activation('softmax')) \n",
    "# softmax is here because we have 10 classes and we want the outputs to be the probability that \n",
    "# the input encountered is that class\n",
    "\n",
    "# conv2d model #\n",
    "modelconv = Sequential()\n",
    "modelconv.add(Conv2D(32, (3,3), input_shape=(28,28,1)))\n",
    "modelconv.add(Conv2D(32, (3,3)))\n",
    "modelconv.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelconv.add(Dropout(0.4))\n",
    "\n",
    "modelconv.add(Conv2D(64, (3,3)))\n",
    "modelconv.add(Conv2D(64, (3,3)))\n",
    "modelconv.add(MaxPooling2D(pool_size=(2,2)))\n",
    "modelconv.add(Dropout(0.4))\n",
    "\n",
    "modelconv.add(Flatten()) # always flatten when connecting a maxpooling/conv2d layer to a dense layer\n",
    "modelconv.add(Dense(128))\n",
    "modelconv.add(Activation('relu'))\n",
    "modelconv.add(Dropout(0.4))\n",
    "modelconv.add(Dense(128))\n",
    "modelconv.add(Activation('relu'))\n",
    "modelconv.add(Dense(10))\n",
    "modelconv.add(Activation('softmax'))\n",
    "\n",
    "modeldense.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelconv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(modeldense.summary())\n",
    "print('\\n'*4)\n",
    "print(modelconv.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to one-hot encode our labels, because we have 10 classes as our output, and we want the labels to also have 10 dimensions to correspond to the final output layer of our NN.\n",
    "\n",
    "After that, we will preprocess our X data. We will normalize it by dividing by 255 (the max value of a pixel for our X data), so that the range of pixel values is [0,1]. Normalizing makes our NN train more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING OUR DATA\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# ONE-HOT ENCODING OUR LABELS\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "X_train = X_train/255 # normalize our data\n",
    "X_test = X_test/255\n",
    "\n",
    "X_train = np.reshape(X_train, (-1, 28, 28, 1)) # reshaping to fit convolutional model\n",
    "X_test = np.reshape(X_test, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have a validation set and a test set? \n",
    "https://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-netwo\n",
    "\n",
    "**Training Set**: this data set is used to adjust the weights on the neural network.\n",
    "\n",
    "**Validation Set**: this data set is used to minimize overfitting. You're not adjusting the weights of the network with this data set, you're just verifying that any increase in accuracy over the training data set actually yields an increase in accuracy over a data set that has not been shown to the network before, or at least the network hasn't trained on it (i.e. validation data set). If the accuracy over the training data set increases, but the accuracy over the validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.\n",
    "\n",
    "**Testing Set**: this data set is used only for testing the final solution in order to confirm the actual predictive power of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use callbacks to improve the quality of training on our model.\n",
    "\n",
    "Earlystopping will stop model from training after `val_loss` does not improve for a certain number of epochs (based on `patience`).\n",
    "\n",
    "Reducing the Learning Rate on Plateau will decrease the learning rate when `val_loss` does not improve, so that we can reach the local minima better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using callbacks to improve fitting of the model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', \n",
    "                         patience=5,\n",
    "                         )\n",
    "\n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', \n",
    "                             factor=0.2, \n",
    "                             patience=3,\n",
    "                            )\n",
    "\n",
    "callbacks = [earlystop, reduceLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 10s 218us/step - loss: 0.3051 - acc: 0.9032 - val_loss: 0.0800 - val_acc: 0.9757\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 9s 202us/step - loss: 0.1472 - acc: 0.9552 - val_loss: 0.0656 - val_acc: 0.9797\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.1290 - acc: 0.9615 - val_loss: 0.0648 - val_acc: 0.9810\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 9s 196us/step - loss: 0.1175 - acc: 0.9663 - val_loss: 0.0552 - val_acc: 0.9827\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.1086 - acc: 0.9677 - val_loss: 0.0537 - val_acc: 0.9837\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 9s 195us/step - loss: 0.1056 - acc: 0.9679 - val_loss: 0.0534 - val_acc: 0.9828\n",
      "Epoch 7/50\n",
      "45000/45000 [==============================] - 9s 192us/step - loss: 0.0975 - acc: 0.9709 - val_loss: 0.0441 - val_acc: 0.9868\n",
      "Epoch 8/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0993 - acc: 0.9706 - val_loss: 0.0447 - val_acc: 0.9871\n",
      "Epoch 9/50\n",
      "45000/45000 [==============================] - 9s 195us/step - loss: 0.0953 - acc: 0.9722 - val_loss: 0.0441 - val_acc: 0.9867\n",
      "Epoch 10/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0930 - acc: 0.9723 - val_loss: 0.0437 - val_acc: 0.9862\n",
      "Epoch 11/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0927 - acc: 0.9726 - val_loss: 0.0465 - val_acc: 0.9860\n",
      "Epoch 12/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.0921 - acc: 0.9727 - val_loss: 0.0506 - val_acc: 0.9842\n",
      "Epoch 13/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.0837 - acc: 0.9756 - val_loss: 0.0500 - val_acc: 0.9847\n",
      "Epoch 14/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.0612 - acc: 0.9819 - val_loss: 0.0303 - val_acc: 0.9905\n",
      "Epoch 15/50\n",
      "45000/45000 [==============================] - 9s 196us/step - loss: 0.0502 - acc: 0.9846 - val_loss: 0.0311 - val_acc: 0.9901\n",
      "Epoch 16/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0456 - acc: 0.9868 - val_loss: 0.0304 - val_acc: 0.9903\n",
      "Epoch 17/50\n",
      "45000/45000 [==============================] - 9s 198us/step - loss: 0.0447 - acc: 0.9860 - val_loss: 0.0299 - val_acc: 0.9909\n",
      "Epoch 18/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.0288 - val_acc: 0.9923\n",
      "Epoch 19/50\n",
      "45000/45000 [==============================] - 9s 195us/step - loss: 0.0419 - acc: 0.9870 - val_loss: 0.0279 - val_acc: 0.9922\n",
      "Epoch 20/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0412 - acc: 0.9869 - val_loss: 0.0300 - val_acc: 0.9912\n",
      "Epoch 21/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0417 - acc: 0.9869 - val_loss: 0.0280 - val_acc: 0.9916\n",
      "Epoch 22/50\n",
      "45000/45000 [==============================] - 9s 192us/step - loss: 0.0381 - acc: 0.9877 - val_loss: 0.0294 - val_acc: 0.9914\n",
      "Epoch 23/50\n",
      "45000/45000 [==============================] - 9s 192us/step - loss: 0.0343 - acc: 0.9893 - val_loss: 0.0271 - val_acc: 0.9927\n",
      "Epoch 24/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.0317 - acc: 0.9903 - val_loss: 0.0267 - val_acc: 0.9925\n",
      "Epoch 25/50\n",
      "45000/45000 [==============================] - 9s 196us/step - loss: 0.0312 - acc: 0.9895 - val_loss: 0.0287 - val_acc: 0.9915\n",
      "Epoch 26/50\n",
      "45000/45000 [==============================] - 9s 193us/step - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0272 - val_acc: 0.9919\n",
      "Epoch 27/50\n",
      "45000/45000 [==============================] - 9s 197us/step - loss: 0.0307 - acc: 0.9899 - val_loss: 0.0259 - val_acc: 0.9925\n",
      "Epoch 28/50\n",
      "45000/45000 [==============================] - 9s 203us/step - loss: 0.0301 - acc: 0.9899 - val_loss: 0.0263 - val_acc: 0.9927\n",
      "Epoch 29/50\n",
      "45000/45000 [==============================] - 9s 200us/step - loss: 0.0317 - acc: 0.9896 - val_loss: 0.0254 - val_acc: 0.9929\n",
      "Epoch 30/50\n",
      "45000/45000 [==============================] - 9s 200us/step - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0257 - val_acc: 0.9927\n",
      "Epoch 31/50\n",
      "45000/45000 [==============================] - 9s 205us/step - loss: 0.0299 - acc: 0.9900 - val_loss: 0.0254 - val_acc: 0.9927\n",
      "Epoch 32/50\n",
      "45000/45000 [==============================] - 9s 204us/step - loss: 0.0304 - acc: 0.9906 - val_loss: 0.0262 - val_acc: 0.9923\n",
      "Epoch 33/50\n",
      "45000/45000 [==============================] - 9s 204us/step - loss: 0.0297 - acc: 0.9906 - val_loss: 0.0262 - val_acc: 0.9923\n",
      "Epoch 34/50\n",
      "45000/45000 [==============================] - 9s 194us/step - loss: 0.0308 - acc: 0.9906 - val_loss: 0.0264 - val_acc: 0.9923\n",
      "Epoch 35/50\n",
      "45000/45000 [==============================] - 9s 191us/step - loss: 0.0300 - acc: 0.9903 - val_loss: 0.0266 - val_acc: 0.9923\n",
      "Epoch 36/50\n",
      "45000/45000 [==============================] - 9s 191us/step - loss: 0.0296 - acc: 0.9906 - val_loss: 0.0266 - val_acc: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fdfb52f390>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelconv.fit(X_train, y_train, epochs=EPOCHS, callbacks=callbacks, validation_data=(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "45000/45000 [==============================] - 6s 133us/step - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0678 - val_acc: 0.9864\n",
      "Epoch 2/50\n",
      "45000/45000 [==============================] - 6s 134us/step - loss: 0.0087 - acc: 0.9971 - val_loss: 0.0694 - val_acc: 0.9865\n",
      "Epoch 3/50\n",
      "45000/45000 [==============================] - 6s 136us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0711 - val_acc: 0.9863\n",
      "Epoch 4/50\n",
      "45000/45000 [==============================] - 7s 145us/step - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0709 - val_acc: 0.9864\n",
      "Epoch 5/50\n",
      "45000/45000 [==============================] - 6s 139us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0707 - val_acc: 0.9864\n",
      "Epoch 6/50\n",
      "45000/45000 [==============================] - 6s 135us/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0704 - val_acc: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fdfb501e10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldense.fit(X_train, y_train, epochs=EPOCHS, callbacks=callbacks, validation_data=(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our models\n",
    "modelconv.save('convolutional_mnist_digit.h5')\n",
    "modeldense.save('dense_mnist_digit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.022484434038856124, 0.9936]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelconv.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07698074098548184, 0.9869]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeldense.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the evaluation score, we can see that the Convolutional Neural Network is better at identifying handwritten digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
